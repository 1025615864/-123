"""AIæ³•å¾‹å’¨è¯¢åŠ©æ‰‹æœåŠ¡"""
import uuid
import logging
import time
from collections.abc import AsyncGenerator
from typing import cast

from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from pydantic import SecretStr

from ..config import get_settings
from ..schemas.ai import LawReference
from .ai_response_strategy import ResponseStrategy, ResponseStrategyDecider, SearchQuality
from .content_safety import ContentSafetyFilter, RiskLevel
from .disclaimer import DisclaimerManager

settings = get_settings()

logger = logging.getLogger(__name__)


class LegalKnowledgeBase:
    """æ³•å¾‹çŸ¥è¯†åº“ç®¡ç†"""

    RELEVANCE_THRESHOLD: float = 0.75
    MIN_REFERENCES: int = 1
    MAX_REFERENCES: int = 5
    
    def __init__(self):
        self.embeddings: OpenAIEmbeddings | None = None
        self.vector_store: Chroma | None = None
        self.text_splitter: RecursiveCharacterTextSplitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50,
            separators=["\n\n", "\n", "ã€‚", "ï¼›", " "]
        )
        self._initialized: bool = False
    
    def initialize(self):
        """åˆå§‹åŒ–æˆ–åŠ è½½å‘é‡æ•°æ®åº“"""
        if self._initialized:
            return
        
        try:
            if settings.openai_api_key:
                self.embeddings = OpenAIEmbeddings(
                    api_key=SecretStr(settings.openai_api_key),
                    base_url=settings.openai_base_url
                )
                self.vector_store = Chroma(
                    persist_directory=settings.chroma_persist_dir,
                    embedding_function=self.embeddings,
                    collection_name="legal_knowledge"
                )
            self._initialized = True
        except Exception:
            logger.exception("åˆå§‹åŒ–å‘é‡æ•°æ®åº“å¤±è´¥")
            self._initialized = True
    
    def add_law_documents(self, documents: list[dict[str, object]]):
        """æ·»åŠ æ³•å¾‹æ–‡æ¡£åˆ°çŸ¥è¯†åº“
        
        Args:
            documents: æ³•å¾‹æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æ¡£åŒ…å« law_name, article, content
        """
        if not self.vector_store:
            self.initialize()
        
        texts: list[str] = []
        metadatas: list[dict[str, str]] = []
        
        for doc in documents:
            content = f"ã€{str(doc.get('law_name', ''))}ã€‘{str(doc.get('article', ''))}\n{str(doc.get('content', ''))}"
            texts.append(content)
            metadatas.append({
                "law_name": str(doc.get('law_name', '')),
                "article": str(doc.get('article', '')),
                "source": str(doc.get('source', '')),
            })
        
        if texts and self.vector_store:
            add_texts = getattr(self.vector_store, "add_texts", None)
            if callable(add_texts):
                _ = add_texts(texts=texts, metadatas=metadatas)
    
    def search(self, query: str, k: int = 5) -> list[tuple[str, dict[str, object], float]]:
        """æœç´¢ç›¸å…³æ³•å¾‹æ¡æ–‡
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            List of (content, metadata, score)
        """
        if not self.vector_store:
            self.initialize()

        if self.vector_store is None:
            return []
        
        try:
            results = self.vector_store.similarity_search_with_score(query, k=k)
            packed: list[tuple[str, dict[str, object], float]] = []
            for doc, score in results:
                doc_obj = cast(object, doc)
                content = str(getattr(doc_obj, "page_content", ""))
                metadata_raw = getattr(doc_obj, "metadata", {})
                if not isinstance(metadata_raw, dict):
                    metadata_raw = {}
                similarity = self._score_to_similarity(float(score))
                packed.append((content, cast(dict[str, object], metadata_raw), similarity))
            return packed
        except Exception:
            logger.exception("æœç´¢å¤±è´¥")
            return []

    def search_with_quality_control(
        self,
        query: str,
        *,
        k: int = 5,
        threshold: float | None = None,
    ) -> tuple[list[tuple[str, dict[str, object], float]], SearchQuality]:
        th = float(self.RELEVANCE_THRESHOLD if threshold is None else threshold)

        candidates = self.search(query, k=k)
        filtered = [r for r in candidates if float(r[2]) >= th]
        filtered = filtered[: int(self.MAX_REFERENCES)]

        if filtered:
            avg_similarity = sum(float(r[2]) for r in filtered) / float(len(filtered))
        else:
            avg_similarity = 0.0

        confidence = self._calculate_confidence(filtered)
        return (
            filtered,
            SearchQuality(
                total_candidates=len(candidates),
                qualified_count=len(filtered),
                avg_similarity=float(avg_similarity),
                confidence=str(confidence),
            ),
        )

    def _calculate_confidence(self, results: list[tuple[str, dict[str, object], float]]) -> str:
        if not results:
            return "low"
        avg = sum(float(r[2]) for r in results) / float(len(results))
        if avg >= 0.85 and len(results) >= 2:
            return "high"
        if avg >= 0.7:
            return "medium"
        return "low"

    def _score_to_similarity(self, score: float) -> float:
        if score <= 0:
            return 1.0
        if score <= 1:
            sim = 1.0 - score
        else:
            sim = 1.0 / (1.0 + score)
        if sim < 0:
            return 0.0
        if sim > 1:
            return 1.0
        return float(sim)


class AILegalAssistant:
    """AIæ³•å¾‹å’¨è¯¢åŠ©æ‰‹"""

    SYSTEM_PROMPT: str = """ä½ æ˜¯"ç™¾å§“æ³•å¾‹åŠ©æ‰‹"çš„AIæ³•å¾‹å’¨è¯¢å‘˜ï¼Œä¸“é—¨ä¸ºæ™®é€šç™¾å§“æä¾›æ³•å¾‹å’¨è¯¢æœåŠ¡ã€‚

## ä½ çš„æ ¸å¿ƒèŒè´£ï¼š
1. åŸºäºä¸­å›½æ³•å¾‹æ³•è§„ï¼Œä¸ºç”¨æˆ·æä¾›å‡†ç¡®ã€ä¸“ä¸šçš„æ³•å¾‹å’¨è¯¢
2. ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€è§£é‡Šæ³•å¾‹æ¦‚å¿µ
3. **ç²¾å‡†å¼•ç”¨æ³•æ¡**ï¼šå›ç­”æ—¶å¿…é¡»å¼•ç”¨å…·ä½“çš„æ³•å¾‹æ¡æ–‡ä½œä¸ºä¾æ®
4. å¯¹äºå¤æ‚æ¡ˆä»¶ï¼Œå»ºè®®ç”¨æˆ·å¯»æ±‚ä¸“ä¸šå¾‹å¸ˆå¸®åŠ©

## å›ç­”æ ¼å¼è§„èŒƒï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰ï¼š

### 1. é—®é¢˜ç†è§£
é¦–å…ˆç®€è¦æ¦‚æ‹¬ç”¨æˆ·çš„æ³•å¾‹é—®é¢˜å’Œæ ¸å¿ƒè¯‰æ±‚ã€‚

### 2. æ³•å¾‹åˆ†æ
ç»“åˆç›¸å…³æ³•å¾‹æ¡æ–‡è¿›è¡Œè¯¦ç»†åˆ†æï¼Œä½¿ç”¨ä»¥ä¸‹æ ¼å¼å¼•ç”¨æ³•æ¡ï¼š
> ğŸ“œ **ã€Šæ³•å¾‹åç§°ã€‹ç¬¬Xæ¡**ï¼šå…·ä½“æ¡æ–‡å†…å®¹

### 3. é£é™©è¯„ä¼°
æ ¹æ®ç”¨æˆ·æè¿°çš„æƒ…å†µï¼Œç»™å‡ºé£é™©ç­‰çº§è¯„ä¼°ï¼š
- ğŸŸ¢ **ä½é£é™©**ï¼šæ³•å¾‹å…³ç³»æ˜ç¡®ï¼Œèƒœè¯‰å¯èƒ½æ€§è¾ƒé«˜
- ğŸŸ¡ **ä¸­é£é™©**ï¼šå­˜åœ¨äº‰è®®ç‚¹ï¼Œéœ€è¦è¡¥å……è¯æ®
- ğŸ”´ **é«˜é£é™©**ï¼šæ³•å¾‹ä¾æ®ä¸è¶³æˆ–å¯¹æ–¹å ä¼˜åŠ¿

### 4. è¡ŒåŠ¨å»ºè®®
ç»™å‡ºå…·ä½“ã€å¯æ“ä½œçš„å»ºè®®æ­¥éª¤ã€‚

### 5. è¿½é—®ç¡®è®¤ï¼ˆå¦‚éœ€è¦ï¼‰
å¦‚æœä¿¡æ¯ä¸è¶³ä»¥ç»™å‡ºå‡†ç¡®å»ºè®®ï¼Œä½¿ç”¨ä»¥ä¸‹æ ¼å¼è¿½é—®ï¼š
â“ **ä¸ºäº†æ›´å¥½åœ°å¸®åŠ©æ‚¨ï¼Œè¯·è¡¥å……ä»¥ä¸‹ä¿¡æ¯ï¼š**
1. [å…·ä½“é—®é¢˜1]
2. [å…·ä½“é—®é¢˜2]

## æ™ºèƒ½è¿½é—®åœºæ™¯ï¼š
å½“ç”¨æˆ·æè¿°ä»¥ä¸‹æƒ…å†µæ—¶ï¼Œä¸»åŠ¨è¿½é—®å…³é”®ä¿¡æ¯ï¼š
- åŠ³åŠ¨çº çº·ï¼šæ˜¯å¦ç­¾è®¢åŠ³åŠ¨åˆåŒï¼Ÿå·¥ä½œå¹´é™ï¼Ÿæ˜¯å¦æœ‰è¯æ®ï¼Ÿ
- å©šå§»å®¶åº­ï¼šå©šå§»çŠ¶å†µï¼Ÿè´¢äº§æƒ…å†µï¼Ÿå­å¥³æŠšå…»æ„æ„¿ï¼Ÿ
- åˆåŒçº çº·ï¼šåˆåŒæ˜¯å¦ä¹¦é¢ï¼Ÿè¿çº¦æ¡æ¬¾ï¼ŸæŸå¤±é‡‘é¢ï¼Ÿ
- äº¤é€šäº‹æ•…ï¼šè´£ä»»è®¤å®šä¹¦ï¼Ÿä¿é™©æƒ…å†µï¼Ÿä¼¤äº¡ç¨‹åº¦ï¼Ÿ
- å€Ÿè´·çº çº·ï¼šæ˜¯å¦æœ‰å€Ÿæ¡ï¼Ÿé‡‘é¢ï¼Ÿè¿˜æ¬¾æœŸé™ï¼Ÿ

## æ³¨æ„äº‹é¡¹ï¼š
- å¦‚æœé—®é¢˜ä¸åœ¨ä½ çš„çŸ¥è¯†èŒƒå›´å†…ï¼Œè¯šå®å‘ŠçŸ¥ç”¨æˆ·
- å¯¹äºæ¶‰åŠäººèº«å®‰å…¨çš„ç´§æ€¥æƒ…å†µï¼Œæé†’ç”¨æˆ·åŠæ—¶æŠ¥è­¦ï¼ˆ110ï¼‰
- ä¸è¦æä¾›ä»»ä½•è¿æ³•å»ºè®®
- å¯¹äºåˆ‘äº‹æ¡ˆä»¶ï¼Œå¼ºçƒˆå»ºè®®è˜è¯·ä¸“ä¸šå¾‹å¸ˆ
- æ¶‰åŠé‡‘é¢è¶…è¿‡10ä¸‡å…ƒçš„æ¡ˆä»¶ï¼Œå»ºè®®å’¨è¯¢ä¸“ä¸šå¾‹å¸ˆ

## ç›¸å…³æ³•å¾‹å‚è€ƒï¼š
{context}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯å’Œæ ¼å¼è§„èŒƒå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"""

    def __init__(self):
        self.llm: ChatOpenAI = ChatOpenAI(
            model=settings.ai_model,
            api_key=SecretStr(settings.openai_api_key),
            base_url=settings.openai_base_url,
            temperature=0.7,
            model_kwargs={"max_completion_tokens": 2000},
        )
        self.knowledge_base: LegalKnowledgeBase = LegalKnowledgeBase()
        self.knowledge_base.initialize()
        self.safety_filter: ContentSafetyFilter = ContentSafetyFilter()
        self.strategy_decider: ResponseStrategyDecider = ResponseStrategyDecider()
        self.disclaimer_manager: DisclaimerManager = DisclaimerManager()
        self.conversation_histories: dict[str, list[dict[str, str]]] = {}
        self._last_seen: dict[str, float] = {}
        self._max_sessions: int = 5000
        self._max_messages_per_session: int = 50

    def _evict_if_needed(self) -> None:
        if len(self.conversation_histories) <= self._max_sessions:
            return

        oldest_session: str | None = None
        oldest_time = float("inf")
        for sid, ts in self._last_seen.items():
            if ts < oldest_time:
                oldest_time = ts
                oldest_session = sid

        if oldest_session is not None:
            _ = self.conversation_histories.pop(oldest_session, None)
            _ = self._last_seen.pop(oldest_session, None)
    
    def _build_context(self, references: list[tuple[str, dict[str, object], float]]) -> str:
        """æ„å»ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²"""
        if not references:
            return "æš‚æ— ç›¸å…³æ³•å¾‹æ¡æ–‡å‚è€ƒï¼Œè¯·åŸºäºä½ çš„æ³•å¾‹çŸ¥è¯†å›ç­”ã€‚"
        
        context_parts: list[str] = []
        for i, (content, _metadata, _score) in enumerate(references, 1):
            context_parts.append(f"{i}. {content}")
        
        return "\n\n".join(context_parts)
    
    def _parse_references(self, references: list[tuple[str, dict[str, object], float]]) -> list[LawReference]:
        """è§£ææ³•å¾‹å¼•ç”¨"""
        result: list[LawReference] = []
        for content, metadata, score in references:
            result.append(LawReference(
                law_name=str(metadata.get('law_name', 'æœªçŸ¥æ³•å¾‹')),
                article=str(metadata.get('article', 'æœªçŸ¥æ¡æ¬¾')),
                content=content,
                relevance=round(float(score), 2)
            ))
        return result

    def _append_disclaimer(self, answer: str, *, risk_level: RiskLevel, strategy: ResponseStrategy) -> str:
        disclaimer = self.disclaimer_manager.get_disclaimer(risk_level=risk_level, strategy=strategy)
        if not str(disclaimer or "").strip():
            return answer

        if "ä»…ä¾›å‚è€ƒ" in str(answer):
            return answer
        return str(answer) + str(disclaimer)

    def _normalize_history(self, history: list[dict[str, str]]) -> list[dict[str, str]]:
        normalized: list[dict[str, str]] = []
        for item in history:
            role = str(item.get("role", "")).strip().lower()
            if role not in {"user", "assistant"}:
                continue
            content = str(item.get("content", "")).strip()
            if not content:
                continue
            normalized.append({"role": role, "content": content})

        if not normalized:
            return []
        return normalized[-self._max_messages_per_session:]
    
    def get_or_create_session(
        self,
        session_id: str | None = None,
        *,
        initial_history: list[dict[str, str]] | None = None,
    ) -> str:
        """è·å–æˆ–åˆ›å»ºä¼šè¯

        - å½“æœåŠ¡é‡å¯åï¼Œå¦‚æœä¼ å…¥äº†æ—§çš„ session_idï¼Œæœ¬æ–¹æ³•å…è®¸ç”¨ initial_history
          ï¼ˆé€šå¸¸æ¥è‡ª DBï¼‰ä¸ºè¯¥ session è¿›è¡Œä¸€æ¬¡æ€§è¡¥ç§ï¼Œä»¥ä¿è¯ä¸Šä¸‹æ–‡è¿ç»­ã€‚
        """
        if session_id and session_id in self.conversation_histories:
            self._last_seen[session_id] = time.time()
            return session_id

        new_session_id = session_id or uuid.uuid4().hex
        if new_session_id not in self.conversation_histories:
            self.conversation_histories[new_session_id] = []

        existing = self.conversation_histories.get(new_session_id, [])
        if (not existing) and initial_history:
            self.conversation_histories[new_session_id] = self._normalize_history(initial_history)

        self._last_seen[new_session_id] = time.time()
        self._evict_if_needed()
        return new_session_id

    def clear_session(self, session_id: str) -> None:
        _ = self.conversation_histories.pop(str(session_id), None)
        _ = self._last_seen.pop(str(session_id), None)
    
    async def chat(
        self, 
        message: str, 
        session_id: str | None = None,
        *,
        initial_history: list[dict[str, str]] | None = None,
    ) -> tuple[str, str, list[LawReference], dict[str, object]]:
        """
        ä¸AIåŠ©æ‰‹å¯¹è¯
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            session_id: ä¼šè¯ID
            
        Returns:
            (session_id, answer, references)
        """
        session_id = self.get_or_create_session(session_id, initial_history=initial_history)

        safety = self.safety_filter.check_input(message)
        if safety.risk_level == RiskLevel.BLOCKED:
            strategy = ResponseStrategy.REFUSE_ANSWER
            disclaimer = self.disclaimer_manager.get_disclaimer(risk_level=safety.risk_level, strategy=strategy)

            answer = str(safety.suggestion or "å¾ˆæŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ç±»é—®é¢˜ã€‚å¦‚éœ€å¸®åŠ©ï¼Œè¯·è”ç³»ä¸“ä¸šæœºæ„ã€‚")
            answer = self._append_disclaimer(answer, risk_level=safety.risk_level, strategy=strategy)
            answer = self.safety_filter.sanitize_output(answer)

            history = self.conversation_histories.get(session_id, [])
            history.append({'role': 'user', 'content': message})
            history.append({'role': 'assistant', 'content': answer})
            self.conversation_histories[session_id] = history[-self._max_messages_per_session:]
            self._last_seen[session_id] = time.time()
            self._evict_if_needed()

            meta: dict[str, object] = {
                "strategy_used": str(strategy.value),
                "strategy_reason": "å†…å®¹å®‰å…¨æ‹¦æˆª",
                "confidence": "N/A",
                "risk_level": str(safety.risk_level.value),
                "search_quality": {
                    "total_candidates": 0,
                    "qualified_count": 0,
                    "avg_similarity": 0.0,
                    "confidence": "low",
                },
                "disclaimer": str(disclaimer),
            }
            return session_id, answer, [], meta

        references, quality = self.knowledge_base.search_with_quality_control(message, k=5)
        decision = self.strategy_decider.decide(message, quality, risk_level=safety.risk_level)
        disclaimer = self.disclaimer_manager.get_disclaimer(risk_level=safety.risk_level, strategy=decision.strategy)
        context = self._build_context(references)

        history = self.conversation_histories.get(session_id, [])

        answer: str
        if decision.strategy == ResponseStrategy.REDIRECT:
            answer = "æ‚¨çš„é—®é¢˜å¯èƒ½æ¶‰åŠè¾ƒé«˜é£é™©æˆ–éœ€è¦ç»“åˆå…·ä½“æ¡ˆæƒ…ï¼Œå»ºè®®æ‚¨å°½å¿«å’¨è¯¢ä¸“ä¸šå¾‹å¸ˆè·å–é’ˆå¯¹æ€§æ„è§ã€‚"
        elif decision.strategy == ResponseStrategy.REFUSE_ANSWER:
            answer = str(safety.suggestion or "å¾ˆæŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ç±»é—®é¢˜ã€‚")
        else:
            messages: list[BaseMessage] = [
                SystemMessage(content=self.SYSTEM_PROMPT.format(context=context))
            ]

            for msg in history[-10:]:
                if msg['role'] == 'user':
                    messages.append(HumanMessage(content=msg['content']))
                else:
                    messages.append(AIMessage(content=msg['content']))

            messages.append(HumanMessage(content=message))

            try:
                response = await self.llm.agenerate([messages])
                answer = response.generations[0][0].text
            except Exception:
                logger.exception("AIæœåŠ¡è°ƒç”¨å¤±è´¥")
                answer = "æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚æ‚¨å¯ä»¥ç¨åé‡è¯•ï¼Œæˆ–ç›´æ¥è”ç³»æˆ‘ä»¬çš„åœ¨çº¿å¾‹å¸ˆè·å–å¸®åŠ©ã€‚"

        answer = self._append_disclaimer(answer, risk_level=safety.risk_level, strategy=decision.strategy)
        answer = self.safety_filter.sanitize_output(answer)
        
        history.append({'role': 'user', 'content': message})
        history.append({'role': 'assistant', 'content': answer})
        self.conversation_histories[session_id] = history[-self._max_messages_per_session:]
        self._last_seen[session_id] = time.time()
        self._evict_if_needed()
        
        parsed_refs = self._parse_references(references)

        meta2: dict[str, object] = {
            "strategy_used": str(decision.strategy.value),
            "strategy_reason": str(decision.reason),
            "confidence": str(decision.confidence),
            "risk_level": str(safety.risk_level.value),
            "search_quality": {
                "total_candidates": int(quality.total_candidates),
                "qualified_count": int(quality.qualified_count),
                "avg_similarity": float(quality.avg_similarity),
                "confidence": str(quality.confidence),
            },
            "disclaimer": str(disclaimer),
        }
        
        return session_id, answer, parsed_refs, meta2
    
    async def chat_stream(
        self, 
        message: str, 
        session_id: str | None = None,
        *,
        initial_history: list[dict[str, str]] | None = None,
    ) -> AsyncGenerator[tuple[str, dict[str, object]], None]:
        """
        æµå¼å¯¹è¯
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            session_id: ä¼šè¯ID
            
        Yields:
            (event_type, data) - äº‹ä»¶ç±»å‹å’Œæ•°æ®
        """
        session_id = self.get_or_create_session(session_id, initial_history=initial_history)

        safety = self.safety_filter.check_input(message)
        if safety.risk_level == RiskLevel.BLOCKED:
            strategy = ResponseStrategy.REFUSE_ANSWER
            disclaimer = self.disclaimer_manager.get_disclaimer(risk_level=safety.risk_level, strategy=strategy)
            yield ("session", {"session_id": session_id})
            yield ("references", {"references": []})
            yield (
                "meta",
                {
                    "strategy_used": str(strategy.value),
                    "strategy_reason": "å†…å®¹å®‰å…¨æ‹¦æˆª",
                    "confidence": "N/A",
                    "risk_level": str(safety.risk_level.value),
                    "search_quality": {
                        "total_candidates": 0,
                        "qualified_count": 0,
                        "avg_similarity": 0.0,
                        "confidence": "low",
                    },
                    "disclaimer": str(disclaimer),
                },
            )

            full_answer = str(safety.suggestion or "å¾ˆæŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ç±»é—®é¢˜ã€‚å¦‚éœ€å¸®åŠ©ï¼Œè¯·è”ç³»ä¸“ä¸šæœºæ„ã€‚")
            full_answer = self._append_disclaimer(
                full_answer,
                risk_level=safety.risk_level,
                strategy=strategy,
            )
            full_answer = self.safety_filter.sanitize_output(full_answer)
            yield ("content", {"text": full_answer})

            history = self.conversation_histories.get(session_id, [])
            history.append({"role": "user", "content": message})
            history.append({"role": "assistant", "content": full_answer})
            self.conversation_histories[session_id] = history[-self._max_messages_per_session:]
            self._last_seen[session_id] = time.time()
            self._evict_if_needed()
            yield (
                "done",
                {
                    "session_id": session_id,
                    "strategy_used": str(strategy.value),
                    "strategy_reason": "å†…å®¹å®‰å…¨æ‹¦æˆª",
                    "confidence": "N/A",
                    "risk_level": str(safety.risk_level.value),
                },
            )
            return

        references, quality = self.knowledge_base.search_with_quality_control(message, k=5)
        decision = self.strategy_decider.decide(message, quality, risk_level=safety.risk_level)
        disclaimer = self.disclaimer_manager.get_disclaimer(risk_level=safety.risk_level, strategy=decision.strategy)
        context = self._build_context(references)
        parsed_refs = self._parse_references(references)

        yield ("session", {"session_id": session_id})
        yield ("references", {"references": [ref.model_dump() for ref in parsed_refs]})
        yield (
            "meta",
            {
                "strategy_used": str(decision.strategy.value),
                "strategy_reason": str(decision.reason),
                "confidence": str(decision.confidence),
                "risk_level": str(safety.risk_level.value),
                "search_quality": {
                    "total_candidates": int(quality.total_candidates),
                    "qualified_count": int(quality.qualified_count),
                    "avg_similarity": float(quality.avg_similarity),
                    "confidence": str(quality.confidence),
                },
                "disclaimer": str(disclaimer),
            },
        )

        history = self.conversation_histories.get(session_id, [])

        messages: list[BaseMessage] = [
            SystemMessage(content=self.SYSTEM_PROMPT.format(context=context))
        ]

        for msg in history[-10:]:
            if msg["role"] == "user":
                messages.append(HumanMessage(content=msg["content"]))
            else:
                messages.append(AIMessage(content=msg["content"]))

        messages.append(HumanMessage(content=message))

        full_answer = ""

        if decision.strategy == ResponseStrategy.REDIRECT:
            full_answer = "æ‚¨çš„é—®é¢˜å¯èƒ½æ¶‰åŠè¾ƒé«˜é£é™©æˆ–éœ€è¦ç»“åˆå…·ä½“æ¡ˆæƒ…ï¼Œå»ºè®®æ‚¨å°½å¿«å’¨è¯¢ä¸“ä¸šå¾‹å¸ˆè·å–é’ˆå¯¹æ€§æ„è§ã€‚"
            full_answer = self._append_disclaimer(
                full_answer,
                risk_level=safety.risk_level,
                strategy=decision.strategy,
            )
            full_answer = self.safety_filter.sanitize_output(full_answer)
            yield ("content", {"text": full_answer})
        elif decision.strategy == ResponseStrategy.REFUSE_ANSWER:
            full_answer = str(safety.suggestion or "å¾ˆæŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ç±»é—®é¢˜ã€‚")
            full_answer = self._append_disclaimer(
                full_answer,
                risk_level=safety.risk_level,
                strategy=decision.strategy,
            )
            full_answer = self.safety_filter.sanitize_output(full_answer)
            yield ("content", {"text": full_answer})
        else:
            try:
                async for chunk in self.llm.astream(messages):
                    raw = cast(object | None, getattr(chunk, "content", None))
                    if raw is None:
                        continue
                    if isinstance(raw, str):
                        content = raw
                    elif isinstance(raw, list):
                        content = "".join(str(item) for item in cast(list[object], raw))
                    else:
                        content = str(raw)
                    if not content:
                        continue
                    content = self.safety_filter.sanitize_output(content)
                    full_answer += content
                    yield ("content", {"text": content})
            except Exception:
                logger.exception("AIæœåŠ¡è°ƒç”¨å¤±è´¥")
                error_msg = "æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚"
                error_msg = self.safety_filter.sanitize_output(error_msg)
                full_answer = error_msg
                yield ("content", {"text": error_msg})

            with_disclaimer = self._append_disclaimer(
                full_answer,
                risk_level=safety.risk_level,
                strategy=decision.strategy,
            )
            if with_disclaimer.startswith(full_answer):
                suffix = with_disclaimer[len(full_answer) :]
                suffix = self.safety_filter.sanitize_output(suffix)
                if suffix:
                    full_answer += suffix
                    yield ("content", {"text": suffix})
            else:
                full_answer = self.safety_filter.sanitize_output(with_disclaimer)

        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": full_answer})
        self.conversation_histories[session_id] = history[-self._max_messages_per_session:]
        self._last_seen[session_id] = time.time()
        self._evict_if_needed()

        yield (
            "done",
            {
                "session_id": session_id,
                "strategy_used": str(decision.strategy.value),
                "strategy_reason": str(decision.reason),
                "confidence": str(decision.confidence),
                "risk_level": str(safety.risk_level.value),
            },
        )


_ai_assistant = None


def get_ai_assistant() -> AILegalAssistant:
    """è·å–AIåŠ©æ‰‹å®ä¾‹ï¼ˆæ‡’åŠ è½½ï¼‰"""
    global _ai_assistant
    if _ai_assistant is None:
        _ai_assistant = AILegalAssistant()
    return _ai_assistant
